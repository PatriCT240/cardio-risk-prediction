# Cardiovascular Risk Prediction â€“ Endâ€‘toâ€‘End Machine Learning Pipeline

This repository contains a full, productionâ€‘grade machine learning system for predicting **10â€‘year cardiovascular disease (CVD) risk** using structured clinical data.  
The project integrates **clinical data cleaning**, **feature engineering**, **robust preprocessing**, **multiple model families**, **probability calibration**, **threshold optimization**, **interpretability**, and **deployment utilities**.

The codebase is written in **Python**, follows a **modular seniorâ€‘level architecture**, and includes **enterpriseâ€‘style docstrings** with no inline comments.

---

## ğŸš€ Quick Start

Train, evaluate, and generate predictions with a single command sequence:

# 1. Prepare data
python -m src.data_prep

# 2. Train models (standard + advanced)
python -m src.modeling

# 3. Evaluate performance + calibration
python -m src.evaluation

# 4. Generate predictions for new patients
python -m src.predict --input sample.json

---

## ğŸ” Project Overview

The goal is to build a clinically meaningful and statistically robust model capable of estimating the probability of cardiovascular disease based on:

- Demographics  
- Anthropometrics  
- Blood pressure  
- Laboratory markers  
- Lifestyle factors  
- Derived clinical flags  

The project includes:
- A **standard pipeline**
- An **advanced robustness pipeline** with injected missingness and Gaussian noise
- A **calibrated final model** ready for deployment

---

## ğŸ“ Repository Structure

cardio-risk-prediction/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Original dataset (cardio_train.csv)
â”‚   â””â”€â”€ processed/               # Cleaned, engineered, and split datasets
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploratory_analysis.ipynb       # EDA, distributions, clinical cleaning rules
â”‚   â”œâ”€â”€ 02_standard_pipeline.ipynb          # Baseline preprocessing + LR/RF training
â”‚   â”œâ”€â”€ 03_advanced_pipeline.ipynb          # Robust pipeline + HGB training + calibration
â”‚   â”œâ”€â”€ 04_thresholds_calibration.ipynb     # Threshold optimization (Youden, cost-based, topâ€‘k)
â”‚   â””â”€â”€ 05_model_interpretability.ipynb     # PI, PDP, ALE, interactions, SHAP-style analysis
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_prep.py              # Clinical cleaning, feature engineering, noise/missingness injection
â”‚   â”œâ”€â”€ preprocessing.py          # Preprocessing pipelines (standard + advanced)
â”‚   â”œâ”€â”€ modeling.py               # Model training, CV, calibration, model selection
â”‚   â”œâ”€â”€ evaluation.py             # Metrics, bootstrapping, calibration, thresholds, fairness
â”‚   â”œâ”€â”€ interpretability.py       # Permutation importance, subgroup analyses, PDP, SHAP, interactions
â”‚   â”œâ”€â”€ visualization.py          # Plotting utilities (ROC, PR, calibration, SHAP, etc.)
â”‚   â””â”€â”€ config.py                 # Global configuration (paths, seeds, feature groups)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ final_pipeline.joblib     # Final calibrated production model (HGB + robustness)
â”‚
â”œâ”€â”€ model_card/
â”‚   â””â”€â”€ model_card.md             # Full clinical + technical documentation of the model
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ tables/                   # Exported evaluation tables
â”‚   â”œâ”€â”€ figures/                  # Generated plots (ROC, PR, calibration, ALE, etc.)
â”‚   â””â”€â”€ executive_summary.pdf     # High-level summary for stakeholders
â”‚
â”œâ”€â”€ README.md                     # Project overview, installation, usage, structure
â””â”€â”€ requirements.txt              # Python dependencies

---

## âš™ï¸ Installation

git clone https://github.com/PatriCT240/cardio-risk-prediction.git
cd cardio-risk-prediction

python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows

pip install -r requirements.txt

---

## ğŸ“¦ Data

The project uses the **CardioVascular Disease dataset** (70,000 patients).
Place the raw file here:
data/raw/cardio_train.csv

---

## ğŸ§© Configuration

config.py defines:

- Numerical variables for histograms and boxplots
- Categorical variables for EDA and modeling
- Target variable (cardio)
- Humanâ€‘readable category labels
- Global random seed
- Number of CV splits

----

## ğŸ§¼ Data Preparation

data_prep.py performs:

- Strict clinical cleaning
- Winsorization
- Feature engineering (BMI, age bands, hypertension flags, lifestyle flags)
- Missingness injection (10%)
- Gaussian noise injection (5% of std)
- Postâ€‘noise clipping
- Train/test split
- Traceability dictionary

---

## ğŸ”§ Preprocessing

preprocessing.py builds:

- Train/test split with stratification
- Feature group definitions (numerical, ordinal, binary, flags)
- Standard preprocessing pipeline (median imputation, scaling, ordinal encoding, oneâ€‘hot encoding)
- Advanced preprocessing (median) with sparseâ€‘safe scaling
- Advanced preprocessing (KNN) for robustness experiments
- Consistent ColumnTransformer outputs for all models

---

## ğŸ§ª Modeling

modeling.py trains:

- Logistic Regression
- Random Forest
- HistGradientBoosting (advanced model)

It also performs:

- 5â€‘fold stratified crossâ€‘validation
- ROCâ€‘AUC and PRâ€‘AUC evaluation
- Model comparison
- Probability calibration (isotonic)
- Final pipeline assembly

**Final production model:  
HistGradientBoosting + robustness pipeline + isotonic calibration**  
Stored at: models/final_pipeline.joblib

---

## ğŸ“Š Evaluation

evaluation.py includes:

- ROCâ€‘AUC, PRâ€‘AUC
- Bootstrapped confidence intervals
- Reliability curves + Brier score + ECE score
- Threshold selection:
    - Youden J
    - Costâ€‘based (FN:FP = 5:1)
    - Topâ€‘k (20%)
- Subgroup fairness analysis (age Ã— gender)

---

## ğŸ” Interpretability

interpretability.py provides:

- Permutation Importance
- Partial Dependence (PDP)
- SHAP (TreeExplainer)
- SHAP interaction 

---

## ğŸ“Š Visualization

visualization.py generates:

- Histograms with clinical visualization limits
- Categorical barplots with humanâ€‘readable labels
- Boxplots by target
- Correlation matrix
- Category Ã— target heatmaps
- ROC and PR curves
- Calibration plots (reliability + perâ€‘bin ECE)
- Confusion matrix at custom thresholds
- Metrics barplots (sensitivity, specificity, PPV, NPV, F1)
- Permutation Importance
- Partial Dependence (PDP)
- SHAP summary plots
- SHAP dependence plots with automatic feature mapping
- SHAP interaction plots

---

## ğŸ“˜ Model Card

A full clinical and technical description is available in:
model_card/model_card.md

---

## ğŸ§ª Reproducibility

All modules use:

- Fixed random seeds
- Deterministic preprocessing
- Explicit feature groups
- Traceability for missingness and noise

---

## âš™ï¸ Requirements
Python 3.10+  
pandas  
numpy  
matplotlib  
seaborn  
scikitâ€‘learn  

---

## ğŸ“ˆ Key Findings
- HistGradientBoosting + median imputation is the best model.  
- Calibrated probabilities improve clinical reliability.  
- Threshold optimization balances sensitivity and specificity.  
- Interpretability confirms known risk factors (hypertension, age, cholesterol).  
- Fairness analysis reveals subgroup disparities requiring attention.

---

## ğŸ“„ License

This project is released under the MIT License.

---

## ğŸ‘¤ Author

Patricia C. Torrell
Clinical Data Analyst transitioning into Data Analytics & Medical Writing  
Focused on clinical modeling, reproducible pipelines, and interpretable ML.  

**LinkedIn**: [linkedin.com/in/patricia-c-torrell](https://www.linkedin.com/in/patricia-c-torrell)  
**GitHub**: [github.com/PatriCT240.github.io](https://github.com/PatriCT240.github.io)  

---

## ğŸ”‘ Key Takeaways for Recruiters
- **Industryâ€‘grade project architecture** with strict modular separation (`src/` modules, notebooks, reports).  
- **Reproducible and transparent workflow**, with clear saving logic and reporting.  
- **Predictive modeling proficiency**: Logistic Regression, Random Forest, HistGradientBoosting.  
- **Clinical domain expertise**: hypertension, cholesterol, BMI, age bands, lifestyle risk factors.  
- **Professional visualization and reporting layer** with modular plots and consolidated outputs.  
- **Fairness and interpretability focus**, ensuring transparency and equity in predictions.  
- **Clear communication and documentation**, including executive summary and recruiterâ€‘friendly README.

